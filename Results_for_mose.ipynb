{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315f1338-1b53-4aeb-8c55-6c3d0350f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing experiment: MotifLoss_0 ===\n",
      "  Dataset=Benzene, Arch=GCN\n",
      "  Dataset=Benzene, Arch=GAT\n",
      "  Dataset=Benzene, Arch=GIN\n",
      "  Dataset=hERG, Arch=GCN\n",
      "  Dataset=hERG, Arch=GAT\n",
      "  Dataset=hERG, Arch=GIN\n",
      "  Dataset=Mutagenicity, Arch=GCN\n",
      "  Dataset=Mutagenicity, Arch=GAT\n",
      "  Dataset=Mutagenicity, Arch=GIN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GCN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GAT\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GIN\n",
      "  Dataset=Alkane_Carbonyl, Arch=GAT\n",
      "  Dataset=Alkane_Carbonyl, Arch=GIN\n",
      "  Dataset=Alkane_Carbonyl, Arch=GCN\n",
      "  Dataset=BBBP, Arch=GCN\n",
      "  Dataset=BBBP, Arch=GAT\n",
      "  Dataset=BBBP, Arch=GIN\n",
      "  Dataset=Lipophilicity, Arch=GCN\n",
      "  Dataset=Lipophilicity, Arch=GAT\n",
      "  Dataset=Lipophilicity, Arch=GIN\n",
      "  Dataset=esol, Arch=GCN\n",
      "  Dataset=esol, Arch=GAT\n",
      "  Dataset=esol, Arch=GIN\n",
      "\n",
      "=== Processing experiment: MotifLoss_0.0 ===\n",
      "  Dataset=hERG, Arch=GCN\n",
      "  Dataset=hERG, Arch=GAT\n",
      "  Dataset=hERG, Arch=GIN\n",
      "  Dataset=hERG, Arch=SAGE\n",
      "  Dataset=Alkane_Carbonyl, Arch=SAGE\n",
      "  Dataset=Alkane_Carbonyl, Arch=GIN\n",
      "  Dataset=Alkane_Carbonyl, Arch=GAT\n",
      "  Dataset=Alkane_Carbonyl, Arch=GCN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GAT\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GIN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GCN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=SAGE\n",
      "  Dataset=Mutagenicity, Arch=SAGE\n",
      "  Dataset=Mutagenicity, Arch=GIN\n",
      "  Dataset=Mutagenicity, Arch=GAT\n",
      "  Dataset=Mutagenicity, Arch=GCN\n",
      "  Dataset=Lipophilicity, Arch=GCN\n",
      "  Dataset=Lipophilicity, Arch=GAT\n",
      "  Dataset=Lipophilicity, Arch=GIN\n",
      "  Dataset=Lipophilicity, Arch=SAGE\n",
      "  Dataset=Benzene, Arch=SAGE\n",
      "  Dataset=Benzene, Arch=GIN\n",
      "  Dataset=Benzene, Arch=GAT\n",
      "  Dataset=Benzene, Arch=GCN\n",
      "  Dataset=esol, Arch=GCN\n",
      "  Dataset=esol, Arch=GAT\n",
      "  Dataset=esol, Arch=GIN\n",
      "  Dataset=esol, Arch=SAGE\n",
      "  Dataset=BBBP, Arch=GCN\n",
      "  Dataset=BBBP, Arch=GAT\n",
      "  Dataset=BBBP, Arch=GIN\n",
      "  Dataset=BBBP, Arch=SAGE\n",
      "\n",
      "=== Processing experiment: MotifLoss_1 ===\n",
      "  Dataset=Benzene, Arch=GCN\n",
      "  Dataset=Benzene, Arch=GAT\n",
      "  Dataset=Benzene, Arch=GIN\n",
      "  Dataset=Benzene, Arch=SAGE\n",
      "  Dataset=hERG, Arch=GAT\n",
      "  Dataset=hERG, Arch=GIN\n",
      "  Dataset=hERG, Arch=GCN\n",
      "  Dataset=hERG, Arch=SAGE\n",
      "  Dataset=Mutagenicity, Arch=GCN\n",
      "  Dataset=Mutagenicity, Arch=GAT\n",
      "  Dataset=Mutagenicity, Arch=GIN\n",
      "  Dataset=Mutagenicity, Arch=SAGE\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GCN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GAT\n",
      "  Dataset=Fluoride_Carbonyl, Arch=GIN\n",
      "  Dataset=Fluoride_Carbonyl, Arch=SAGE\n",
      "  Dataset=Alkane_Carbonyl, Arch=GCN\n",
      "  Dataset=Alkane_Carbonyl, Arch=GAT\n",
      "  Dataset=Alkane_Carbonyl, Arch=GIN\n",
      "  Dataset=Alkane_Carbonyl, Arch=SAGE\n",
      "  Dataset=BBBP, Arch=GCN\n",
      "  Dataset=BBBP, Arch=GAT\n",
      "  Dataset=BBBP, Arch=GIN\n",
      "  Dataset=BBBP, Arch=SAGE\n",
      "  Dataset=Lipophilicity, Arch=GAT\n",
      "  Dataset=Lipophilicity, Arch=GIN\n",
      "  Dataset=Lipophilicity, Arch=GCN\n",
      "  Dataset=Lipophilicity, Arch=SAGE\n",
      "  Dataset=esol, Arch=GCN\n",
      "  Dataset=esol, Arch=GAT\n",
      "  Dataset=esol, Arch=GIN\n",
      "  Dataset=esol, Arch=SAGE\n",
      "\n",
      "Saved summary to: /nfs/hpc/share/kokatea/ChemIntuit/GSAT/motif_correlation_summary_all_splits.csv\n",
      "Saved motif-level details to: /nfs/hpc/share/kokatea/ChemIntuit/GSAT/motif_correlation_details_all_splits.csv\n",
      "Saved aggregated mean/std of PearsonR to: /nfs/hpc/share/kokatea/ChemIntuit/GSAT/motif_correlation_summary_agg.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"Scalar sigmoid with basic numerical stability.\"\"\"\n",
    "    if x >= 0:\n",
    "        z = math.exp(-x)\n",
    "        return 1.0 / (1.0 + z)\n",
    "    else:\n",
    "        z = math.exp(x)\n",
    "        return z / (1.0 + z)\n",
    "\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def arch_from_dir(model_dir_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert 'modelGIN' -> 'GIN', 'modelGAT' -> 'GAT', etc.\n",
    "    Fallback: return original if it doesn't start with 'model'.\n",
    "    \"\"\"\n",
    "    if model_dir_name.lower().startswith(\"model\"):\n",
    "        return model_dir_name[5:]\n",
    "    return model_dir_name\n",
    "\n",
    "\n",
    "def collect_impacts(impact_file: Path):\n",
    "    \"\"\"\n",
    "    Collect motif impacts from masked-edge-impact.jsonl or masked-impact.jsonl.\n",
    "    Returns:\n",
    "      impacts[(split, motif_idx)] -> list of impact values\n",
    "      graph_ids[(split, motif_idx)] -> set of 'split|graph_idx'\n",
    "    \"\"\"\n",
    "    impacts = defaultdict(list)\n",
    "    graph_ids = defaultdict(set)\n",
    "\n",
    "    for rec in read_jsonl(impact_file):\n",
    "        split = rec.get(\"split\", \"unknown\")\n",
    "\n",
    "        motif_idx = rec.get(\"motif_idx\", rec.get(\"motif_index\", None))\n",
    "        if motif_idx is None or motif_idx < 0:\n",
    "            # ignore UNK / background motif\n",
    "            continue\n",
    "\n",
    "        old_pred = rec[\"old_prediction\"]\n",
    "        new_pred = rec[\"new_prediction\"]\n",
    "        imp = abs(sigmoid(new_pred) - sigmoid(old_pred))\n",
    "\n",
    "        key = (split, motif_idx)\n",
    "        impacts[key].append(imp)\n",
    "\n",
    "        graph_idx = rec.get(\"graph_idx\", None)\n",
    "        if graph_idx is not None:\n",
    "            graph_ids[key].add(f\"{split}|{graph_idx}\")\n",
    "\n",
    "    return impacts, graph_ids\n",
    "\n",
    "\n",
    "def collect_node_scores(node_scores_file: Path):\n",
    "    \"\"\"\n",
    "    Collect motif-level node scores from node_scores.jsonl.\n",
    "    Returns:\n",
    "      scores[(split, motif_index)] -> list of scores\n",
    "      graph_ids[(split, motif_index)] -> set of 'split|smiles'\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    graph_ids = defaultdict(set)\n",
    "\n",
    "    for rec in read_jsonl(node_scores_file):\n",
    "        split = rec.get(\"split\", \"unknown\")\n",
    "\n",
    "        motif_index = rec.get(\"motif_index\", rec.get(\"motif_idx\", None))\n",
    "        if motif_index is None or motif_index < 0:\n",
    "            continue\n",
    "\n",
    "        score = rec[\"score\"]\n",
    "        smiles = rec.get(\"smiles\", None)\n",
    "\n",
    "        key = (split, motif_index)\n",
    "        scores[key].append(score)\n",
    "\n",
    "        if smiles is not None:\n",
    "            graph_ids[key].add(f\"{split}|{smiles}\")\n",
    "\n",
    "    return scores, graph_ids\n",
    "\n",
    "\n",
    "def aggregate_mean(dict_of_lists):\n",
    "    return {k: float(np.mean(v)) for k, v in dict_of_lists.items() if len(v) > 0}\n",
    "\n",
    "\n",
    "def aggregate_counts(dict_of_sets):\n",
    "    return {k: len(v) for k, v in dict_of_sets.items()}\n",
    "\n",
    "\n",
    "def build_corr_df(\n",
    "    avg_scores,\n",
    "    avg_impacts,\n",
    "    cnt_graphs_node,\n",
    "    cnt_graphs_imp,\n",
    "    impact_type: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame with:\n",
    "      split, motif_idx, avg_score, avg_impact, n_graphs_node, n_graphs_imp,\n",
    "      graph_count_match, graph_count_diff, impact_type\n",
    "    Only motifs/splits that appear in both avg_scores and avg_impacts are kept.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    common_keys = set(avg_scores.keys()) & set(avg_impacts.keys())\n",
    "\n",
    "    for (split, motif_idx) in sorted(common_keys):\n",
    "        s = avg_scores[(split, motif_idx)]\n",
    "        imp = avg_impacts[(split, motif_idx)]\n",
    "        n_node = cnt_graphs_node.get((split, motif_idx), 0)\n",
    "        n_imp = cnt_graphs_imp.get((split, motif_idx), 0)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                split=split,\n",
    "                motif_idx=motif_idx,\n",
    "                avg_score=s,\n",
    "                avg_impact=imp,\n",
    "                n_graphs_node=n_node,\n",
    "                n_graphs_imp=n_imp,\n",
    "                graph_count_match=(n_node == n_imp),\n",
    "                graph_count_diff=n_node - n_imp,\n",
    "                impact_type=impact_type,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def compute_corr(df: pd.DataFrame):\n",
    "    \"\"\"Compute Pearson correlation between avg_score and avg_impact.\"\"\"\n",
    "    if df.empty:\n",
    "        return np.nan, np.nan, 0\n",
    "    x = df[\"avg_score\"].values\n",
    "    y = df[\"avg_impact\"].values\n",
    "    # Pearson undefined if no variation or only one point\n",
    "    if len(df) < 2 or np.allclose(x, x[0]) or np.allclose(y, y[0]):\n",
    "        return np.nan, np.nan, len(df)\n",
    "    r, p = pearsonr(x, y)\n",
    "    return float(r), float(p), len(df)\n",
    "\n",
    "\n",
    "def detail_rows_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    experiment_name: str,\n",
    "    dataset_name: str,\n",
    "    arch_name: str,\n",
    "    impact_type: str,\n",
    "):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        rows.append(\n",
    "            dict(\n",
    "                Experiment=experiment_name,\n",
    "                Dataset=dataset_name,\n",
    "                Arch=arch_name,\n",
    "                ImpactType=impact_type,\n",
    "                Split=row[\"split\"],\n",
    "                MotifIdx=int(row[\"motif_idx\"]),\n",
    "                AvgScore=float(row[\"avg_score\"]),\n",
    "                AvgImpact=float(row[\"avg_impact\"]),\n",
    "                NumGraphsNode=int(row[\"n_graphs_node\"]),\n",
    "                NumGraphsImpact=int(row[\"n_graphs_imp\"]),\n",
    "                GraphCountMatch=bool(row[\"graph_count_match\"]),\n",
    "                GraphCountDiff=int(row[\"graph_count_diff\"]),\n",
    "            )\n",
    "        )\n",
    "    return rows\n",
    "\n",
    "\n",
    "def main(root_dir: str):\n",
    "    root = Path(root_dir).resolve()\n",
    "    if not root.exists():\n",
    "        print(f\"Root directory {root} does not exist.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # MotifLoss_* experiment dirs\n",
    "    exp_dirs = sorted(\n",
    "        [p for p in root.iterdir() if p.is_dir() and p.name.startswith(\"MotifLoss_\")]\n",
    "    )\n",
    "    if not exp_dirs:\n",
    "        print(\"No MotifLoss_* directories found under root.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    summary_rows = []\n",
    "    motif_detail_rows = []\n",
    "\n",
    "    for exp_dir in exp_dirs:\n",
    "        experiment_name = exp_dir.name  # e.g. MotifLoss_0\n",
    "        print(f\"\\n=== Processing experiment: {experiment_name} ===\")\n",
    "\n",
    "        # /MotifLoss_k/<dataset>/<modelARCH>/foldX/seedY/\n",
    "        for dataset_dir in exp_dir.iterdir():\n",
    "            if not dataset_dir.is_dir():\n",
    "                continue\n",
    "            dataset_name = dataset_dir.name  # e.g. esol\n",
    "\n",
    "            for model_dir in dataset_dir.iterdir():\n",
    "                if not model_dir.is_dir():\n",
    "                    continue\n",
    "                arch_name = arch_from_dir(model_dir.name)  # e.g. GIN\n",
    "\n",
    "                print(f\"  Dataset={dataset_name}, Arch={arch_name}\")\n",
    "\n",
    "                # Containers aggregated across folds/seeds\n",
    "                edge_impacts_all = defaultdict(list)\n",
    "                edge_graph_ids_all = defaultdict(set)\n",
    "\n",
    "                graph_impacts_all = defaultdict(list)\n",
    "                graph_graph_ids_all = defaultdict(set)\n",
    "\n",
    "                scores_all = defaultdict(list)\n",
    "                score_graph_ids_all = defaultdict(set)\n",
    "\n",
    "                # Traverse folds and seeds\n",
    "                for fold_dir in model_dir.glob(\"fold*\"):\n",
    "                    if not fold_dir.is_dir():\n",
    "                        continue\n",
    "                    for seed_dir in fold_dir.glob(\"seed*\"):\n",
    "                        if not seed_dir.is_dir():\n",
    "                            continue\n",
    "\n",
    "                        edge_impact_file = seed_dir / \"masked-edge-impact.jsonl\"\n",
    "                        graph_impact_file = seed_dir / \"masked-impact.jsonl\"\n",
    "                        node_scores_file = seed_dir / \"node_scores.jsonl\"\n",
    "\n",
    "                        if not node_scores_file.exists():\n",
    "                            print(f\"    [WARN] Missing node_scores in {seed_dir}\")\n",
    "                            continue\n",
    "\n",
    "                        # Node scores\n",
    "                        s, s_graph_ids = collect_node_scores(node_scores_file)\n",
    "                        for k, v in s.items():\n",
    "                            scores_all[k].extend(v)\n",
    "                        for k, v in s_graph_ids.items():\n",
    "                            score_graph_ids_all[k].update(v)\n",
    "\n",
    "                        # Edge-level impacts\n",
    "                        if edge_impact_file.exists():\n",
    "                            e_imp, e_graph = collect_impacts(edge_impact_file)\n",
    "                            for k, v in e_imp.items():\n",
    "                                edge_impacts_all[k].extend(v)\n",
    "                            for k, v in e_graph.items():\n",
    "                                edge_graph_ids_all[k].update(v)\n",
    "                        else:\n",
    "                            print(f\"    [WARN] Missing masked-edge-impact in {seed_dir}\")\n",
    "\n",
    "                        # Graph-level impacts\n",
    "                        if graph_impact_file.exists():\n",
    "                            g_imp, g_graph = collect_impacts(graph_impact_file)\n",
    "                            for k, v in g_imp.items():\n",
    "                                graph_impacts_all[k].extend(v)\n",
    "                            for k, v in g_graph.items():\n",
    "                                graph_graph_ids_all[k].update(v)\n",
    "                        else:\n",
    "                            print(f\"    [WARN] Missing masked-impact in {seed_dir}\")\n",
    "\n",
    "                if not scores_all:\n",
    "                    print(f\"    [INFO] No node scores found; skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Aggregate means & counts\n",
    "                avg_scores = aggregate_mean(scores_all)\n",
    "                count_graphs_node = aggregate_counts(score_graph_ids_all)\n",
    "\n",
    "                avg_edge_impacts = aggregate_mean(edge_impacts_all)\n",
    "                count_graphs_edge = aggregate_counts(edge_graph_ids_all)\n",
    "\n",
    "                avg_graph_impacts = aggregate_mean(graph_impacts_all)\n",
    "                count_graphs_graph = aggregate_counts(graph_graph_ids_all)\n",
    "\n",
    "                # ---- Edge impacts vs scores (masked-edge-impact) ----\n",
    "                if avg_edge_impacts:\n",
    "                    df_edge = build_corr_df(\n",
    "                        avg_scores,\n",
    "                        avg_edge_impacts,\n",
    "                        count_graphs_node,\n",
    "                        count_graphs_edge,\n",
    "                        impact_type=\"masked-edge-impact\",\n",
    "                    )\n",
    "\n",
    "                    # Compute correlation per split\n",
    "                    for split in sorted(df_edge[\"split\"].unique()):\n",
    "                        df_split = df_edge[df_edge[\"split\"] == split]\n",
    "                        r_edge, p_edge, n_edge = compute_corr(df_split)\n",
    "\n",
    "                        summary_rows.append(\n",
    "                            dict(\n",
    "                                Experiment=experiment_name,\n",
    "                                Dataset=dataset_name,\n",
    "                                Arch=arch_name,\n",
    "                                ImpactType=\"masked-edge-impact\",\n",
    "                                Split=split,\n",
    "                                NumMotifs=n_edge,\n",
    "                                PearsonR=r_edge,\n",
    "                                PValue=p_edge,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    motif_detail_rows.extend(\n",
    "                        detail_rows_from_df(\n",
    "                            df_edge,\n",
    "                            experiment_name,\n",
    "                            dataset_name,\n",
    "                            arch_name,\n",
    "                            impact_type=\"masked-edge-impact\",\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # ---- Graph impacts vs scores (masked-impact) ----\n",
    "                if avg_graph_impacts:\n",
    "                    df_graph = build_corr_df(\n",
    "                        avg_scores,\n",
    "                        avg_graph_impacts,\n",
    "                        count_graphs_node,\n",
    "                        count_graphs_graph,\n",
    "                        impact_type=\"masked-impact\",\n",
    "                    )\n",
    "\n",
    "                    for split in sorted(df_graph[\"split\"].unique()):\n",
    "                        df_split = df_graph[df_graph[\"split\"] == split]\n",
    "                        r_graph, p_graph, n_graph = compute_corr(df_split)\n",
    "\n",
    "                        summary_rows.append(\n",
    "                            dict(\n",
    "                                Experiment=experiment_name,\n",
    "                                Dataset=dataset_name,\n",
    "                                Arch=arch_name,\n",
    "                                ImpactType=\"masked-impact\",\n",
    "                                Split=split,\n",
    "                                NumMotifs=n_graph,\n",
    "                                PearsonR=r_graph,\n",
    "                                PValue=p_graph,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    motif_detail_rows.extend(\n",
    "                        detail_rows_from_df(\n",
    "                            df_graph,\n",
    "                            experiment_name,\n",
    "                            dataset_name,\n",
    "                            arch_name,\n",
    "                            impact_type=\"masked-impact\",\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Build DataFrames\n",
    "    # -------------------------\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    detail_df = pd.DataFrame(motif_detail_rows)\n",
    "\n",
    "    # NEW: drop any exact duplicate summary rows\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.drop_duplicates(\n",
    "            subset=[\"Experiment\", \"Dataset\", \"Arch\", \"ImpactType\", \"Split\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "\n",
    "    # NEW: drop any exact duplicate motif-detail rows\n",
    "    if not detail_df.empty:\n",
    "        detail_df = detail_df.drop_duplicates()\n",
    "\n",
    "    summary_out = root / \"motif_correlation_summary_all_splits.csv\"\n",
    "    detail_out = root / \"motif_correlation_details_all_splits.csv\"\n",
    "\n",
    "    summary_df.to_csv(summary_out, index=False)\n",
    "    detail_df.to_csv(detail_out, index=False)\n",
    "\n",
    "    print(f\"\\nSaved summary to: {summary_out}\")\n",
    "    print(f\"Saved motif-level details to: {detail_out}\")\n",
    "\n",
    "    # ==============================\n",
    "    # NEW: mean + std of PearsonR\n",
    "    # ==============================\n",
    "    if not summary_df.empty:\n",
    "        # Group across splits for each (Experiment, Dataset, Arch, ImpactType)\n",
    "        agg_df = (\n",
    "            summary_df\n",
    "            .groupby([\"Experiment\", \"Dataset\", \"Arch\", \"ImpactType\"], as_index=False)\n",
    "            .agg(\n",
    "                NumSplits=(\"PearsonR\", \"count\"),\n",
    "                MeanPearsonR=(\"PearsonR\", \"mean\"),\n",
    "                StdPearsonR=(\"PearsonR\", \"std\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        agg_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"Experiment\",\n",
    "                \"Dataset\",\n",
    "                \"Arch\",\n",
    "                \"ImpactType\",\n",
    "                \"NumSplits\",\n",
    "                \"MeanPearsonR\",\n",
    "                \"StdPearsonR\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # NEW: safety â€” ensure one row per (Experiment, Dataset, Arch, ImpactType)\n",
    "    if not agg_df.empty:\n",
    "        agg_df = agg_df.drop_duplicates(\n",
    "            subset=[\"Experiment\", \"Dataset\", \"Arch\", \"ImpactType\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "\n",
    "    agg_out = root / \"motif_correlation_summary_agg.csv\"\n",
    "    agg_df.to_csv(agg_out, index=False)\n",
    "    print(f\"Saved aggregated mean/std of PearsonR to: {agg_out}\")\n",
    "\n",
    "\n",
    "main(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548fe336-eedc-4355-97c4-a3e404ccec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "csv_path = \"motif_correlation_summary_all_splits.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "out_dir = \"significance_heatmaps\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "split_order = [\"train\", \"valid\", \"test\"]\n",
    "df[\"Split\"] = pd.Categorical(df[\"Split\"], categories=split_order, ordered=True)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "for arch in df[\"Arch\"].unique():\n",
    "    for impact in df[\"ImpactType\"].unique():\n",
    "        sub = df[(df[\"Arch\"] == arch) & (df[\"ImpactType\"] == impact)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        \n",
    "        sub = sub.copy()\n",
    "        sub[\"neglogp\"] = sub[\"PearsonR\"]#-np.log10(sub[\"PearsonR\"].replace(0, 1e-300))\n",
    "        \n",
    "        # Aggregate duplicates by mean\n",
    "        g = sub.groupby([\"Dataset\", \"Split\"])[\"neglogp\"].mean().reset_index()\n",
    "        pivot = g.pivot(index=\"Dataset\", columns=\"Split\", values=\"neglogp\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        im = ax.imshow(pivot, aspect='auto')\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(pivot.columns)))\n",
    "        ax.set_xticklabels(pivot.columns)\n",
    "        ax.set_yticks(np.arange(len(pivot.index)))\n",
    "        ax.set_yticklabels(pivot.index)\n",
    "        \n",
    "        for i in range(len(pivot.index)):\n",
    "            for j in range(len(pivot.columns)):\n",
    "                val = pivot.iloc[i, j]\n",
    "                text = \"NA\" if pd.isna(val) else f\"{val:.2f}\"\n",
    "                ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=8)\n",
    "        \n",
    "        ax.set_title(f\"Significance Heatmap\\nArch={arch}, Impact={impact}\")\n",
    "        fig.colorbar(im, ax=ax, label=\"-log10(p-value)\")\n",
    "        \n",
    "        out_path = os.path.join(out_dir, f\"heatmap_{arch}_{impact}.png\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        saved_files.append(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e790aa2-ebf3-439b-a8b6-dca2104c0700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
